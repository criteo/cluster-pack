name: CI

on:
  push:
    branches: [ master, cherry-pick-release-0.3.17.post1, fork_0.3.17 ]
  pull_request:
    branches: [ master, cherry-pick-release-0.3.17.post1, fork_0.3.17 ]

jobs:
  lint:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2
    - name: Set up Python 3.9
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        enable-cache: true
        cache-dependency-glob: "**/requirements*.txt"
    - name: Install dependencies
      run: |
        uv pip install --system --upgrade pip
        uv pip install --system .
        uv pip install --system -r tests-requirements.txt
    - name: Linter
      run: |
        pylama
    - name: Typer checker
      run: |
        mypy --config-file setup.cfg

  build:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: [3.9, 3.11]
        use-uv: [true, false]

    steps:
    - uses: actions/checkout@v2
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install uv
      if: matrix.use-uv
      uses: astral-sh/setup-uv@v4
      with:
        enable-cache: true
        cache-dependency-glob: "**/requirements*.txt"
    - name: Install dependencies (with uv)
      if: matrix.use-uv
      run: |
        uv pip install --system --upgrade pip
        uv pip install --system -e .
        uv pip install --system -r tests-requirements.txt
    - name: Install dependencies (without uv)
      if: ${{ !matrix.use-uv }}
      run: |
        pip install --upgrade pip
        pip install -e .
        pip install -r tests-requirements.txt
    - name: Tests
      run: |
        pytest -m "not hadoop and not conda" -s tests

  standalone_spark3_with_S3:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        version: [ { python: 3.9.15, spark: 3.5.8 }, { python: 3.11.8, spark: 3.5.8 } ]
        venv_optimization_level: [0, 1, 2]

    steps:
    - uses: actions/checkout@v2

    - name: Prepare uv cache directory
      run: mkdir -p .uv-cache

    - name: Cache uv packages for Docker
      uses: actions/cache@v4
      with:
        path: .uv-cache
        key: uv-docker-${{ matrix.version.python }}
        restore-keys: |
          uv-docker-${{ matrix.version.python }}

    
    - name: Build spark-docker
      run: docker build -t spark-docker ./examples/spark-with-S3 --build-arg SPARK_INPUT_VERSION=${{ matrix.version.spark }} --build-arg PYTHON_VERSION=${{ matrix.version.python }}
    
    - name: Build the docker-compose stack
      run: |
        docker compose -f ./examples/spark-with-S3/docker-compose.yml up -d

    - name: Check running containers
      run: docker ps -a

    - name: Install pyspark
      run: docker exec spark-master ./examples/spark-with-S3/scripts/install_pyspark.sh ${{ matrix.version.spark }}

    - name: Run spark Job with all layout optimization modes
      run: |
        declare -A timings
        for mode in SLOW_FAST_SMALL FAST_MID_BIG MID_SLOW_SMALL DISABLED; do
          echo "=== Testing C_PACK_LAYOUT_OPTIMIZATION=$mode ==="
          start_time=$(date +%s)
          docker exec -e C_PACK_VENV_OPTIMIZATION_LEVEL=${{ matrix.venv_optimization_level }} -e C_PACK_LAYOUT_OPTIMIZATION=$mode spark-master ./examples/spark-with-S3/scripts/run_spark_example.sh
          end_time=$(date +%s)
          timings[$mode]=$((end_time - start_time))
        done
        echo ""
        echo "==========================================="
        echo "TIMING SUMMARY (VENV_OPT=${{ matrix.venv_optimization_level }})"
        echo "==========================================="
        printf "%-20s %10s\n" "Mode" "Time (s)"
        printf "%-20s %10s\n" "--------------------" "----------"
        for mode in SLOW_FAST_SMALL FAST_MID_BIG MID_SLOW_SMALL DISABLED; do
          printf "%-20s %10s\n" "$mode" "${timings[$mode]}"
        done
        echo "==========================================="

  # hadoop_hdfs:
  #   runs-on: ubuntu-latest

  #   steps:
  #   - uses: actions/checkout@v2

  #   - name: Set up Python 3.9
  #     uses: actions/setup-python@v2
  #     with:
  #       python-version: 3.9

  #   - name: Install hadoop-test-cluster
  #     run: |
  #       pip install hadoop-test-cluster

  #   - name: Start cluster
  #     run: |
  #       htcluster startup --image cdh5 --mount .:cluster-pack

  #   - name: Start Job
  #     run: |
  #       # for the hack with script .. see https://github.com/actions/runner/issues/241#issuecomment-577360161
  #       # the prebuild image only contains a conda install, we also install python
  #       # to avoid sharing files on the worker node we copy the python install script via hdfs to worker /tmp folder
  #       script -e -c "htcluster exec -u root -s edge -- chown -R testuser /home/testuser && \
  #                     htcluster exec -u root -s edge -- /home/testuser/cluster-pack/tests/integration/install_python.sh && \                    
  #                     htcluster exec -u root -s edge -- hdfs dfs -put /home/testuser/cluster-pack/tests/integration/install_python.sh hdfs:///tmp && \
  #                     htcluster exec -u root -s worker -- hdfs dfs -get hdfs:///tmp/install_python.sh /home/testuser && \
  #                     htcluster exec -u root -s worker -- chmod +x /home/testuser/install_python.sh && \
  #                     htcluster exec -u root -s worker -- /home/testuser/install_python.sh && \
  #                     htcluster exec -s edge -- /home/testuser/cluster-pack/tests/integration/hadoop_hdfs_tests.sh"